{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 8 - Classification [2] - Multiclass Classification. Regularization. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multiclass Classification.\n",
    "\n",
    "Introduction\n",
    "\n",
    "In binary classification, the objective is to classify data points into 2 categories.\n",
    "\n",
    "In multiclass classification, the objective is to classify them into $K$ categories. In this case we can write $\\mathcal Y = \\{0, 1, \\ldots, K-1\\}$.\n",
    "\n",
    "In this section, we learn several strategies to solve multiclass classification problem.\n",
    "\n",
    "\n",
    "## 1.1 One vs Rest\n",
    "\n",
    "In One Vs Rest (OVR) method (also named OVA- One Vs All), we divide our porblem into $K$ binary classification problems. For each problem $k$ ($k = 0$ to $K-1$), we try to predict whether the data point is more likely to be in category $k$ or not in category $k$. In other words, we try to find $K-1$ predictive hypothesis $y^{(0)}, \\ldots, y^{(K-1)}$ for those $K$ binary classifications problems. This predictive hypothesis (recall that it is a function) is usually expressed in probability form ($P(y|\\mathbf x)$). Finally, we choose the category that maximizes this function.\n",
    "\n",
    "In probabilistic form:\n",
    "\n",
    "$$\n",
    "y^{(0)}(\\mathbf x) = P_0(y = 1_0 | \\mathbf x) \\\\\n",
    "y^{(1)}(\\mathbf x) = P_1(y = 1_1 | \\mathbf x) \\\\\n",
    "\\ldots \\\\\n",
    "y^{(K-1)}(\\mathbf x) = P_{K-1}(y = 1_{K-1} | \\mathbf x) \\\\\n",
    "$$\n",
    "\n",
    "Decide $y(\\mathbf x) = \\arg \\max_k y^{(k)}(x)$\n",
    "\n",
    "where $P_k(y = 1_k | \\mathbf x)$ means the probability of $y = 1_k$ given $\\mathbf x$ in the $k^{th}$ binary classification problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python\n",
    "\n",
    "Python supports OVA for the following models:\n",
    "\n",
    "- **sklearn.svm.LinearSVC** (setting multi_class=\"ovr\") (SVM will be presented in next lecture)\n",
    "- **sklearn.linear_model.LogisticRegression** (setting multi_class=\"ovr\")\n",
    "- **sklearn.linear_model.LogisticRegressionCV** (setting multi_class=\"ovr\")\n",
    "- **sklearn.linear_model.Perceptron** ($\\arg \\max w_k \\cdot x$ instead of probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "We use an example in [1] for fruit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit_label</th>\n",
       "      <th>fruit_name</th>\n",
       "      <th>fruit_subtype</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruit_label fruit_name fruit_subtype  mass  width  height  color_score\n",
       "0            1      apple  granny_smith   192    8.4     7.3         0.55\n",
       "1            1      apple  granny_smith   180    8.0     6.8         0.59\n",
       "2            1      apple  granny_smith   176    7.4     7.2         0.60\n",
       "3            2   mandarin      mandarin    86    6.2     4.7         0.80\n",
       "4            2   mandarin      mandarin    84    6.0     4.6         0.79"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('fruit_data_with_colors.txt', sep = \"\\t\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fruit_name\n",
       "apple       19\n",
       "lemon       16\n",
       "mandarin     5\n",
       "orange      19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('fruit_name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'mandarin', 'orange', 'lemon'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['fruit_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mass  width  height  color_score\n",
       "0   192    8.4     7.3         0.55\n",
       "1   180    8.0     6.8         0.59\n",
       "2   176    7.4     7.2         0.60\n",
       "3    86    6.2     4.7         0.80\n",
       "4    84    6.0     4.6         0.79"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.loc[:, [\"mass\", \"width\", \"height\", \"color_score\"]]\n",
    "y = data.loc[:, \"fruit_label\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.58913538e-01, 5.79678244e-10, 6.41084986e-01, 1.47549370e-06],\n",
       "       [3.80022675e-01, 1.45205396e-02, 1.80228228e-01, 4.25228557e-01],\n",
       "       [6.65960497e-02, 8.14078835e-05, 2.00232934e-01, 7.33089609e-01],\n",
       "       [4.71493119e-01, 2.55799515e-02, 2.29768655e-01, 2.73158275e-01],\n",
       "       [4.26632738e-01, 2.14316517e-03, 4.82735210e-01, 8.84888869e-02],\n",
       "       [5.24513071e-01, 2.08005261e-03, 4.54774529e-01, 1.86323474e-02],\n",
       "       [5.15904203e-01, 6.96963726e-02, 1.01013818e-01, 3.13385606e-01],\n",
       "       [7.80465248e-02, 1.26764354e-05, 4.10608685e-01, 5.11332113e-01],\n",
       "       [5.02069160e-01, 1.25641020e-02, 2.85877856e-01, 1.99488882e-01],\n",
       "       [6.17601418e-01, 7.07567307e-02, 1.72900608e-01, 1.38741242e-01],\n",
       "       [4.20928359e-01, 4.84431271e-01, 5.66334533e-02, 3.80069164e-02],\n",
       "       [4.34393487e-01, 2.95626932e-03, 4.80154371e-01, 8.24958735e-02],\n",
       "       [4.97416822e-01, 8.59337571e-03, 3.31211595e-01, 1.62778207e-01],\n",
       "       [4.61597704e-01, 1.93976127e-03, 4.19746178e-01, 1.16716357e-01],\n",
       "       [4.06680081e-01, 5.68525956e-04, 5.36497009e-01, 5.62543839e-02]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, 1, 3, 1, 1, 4, 1, 1, 2, 3, 1, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 1, 1, 3, 4, 3, 1, 2, 1, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.35005139e-04,  7.90924593e-01, -7.04427722e-01,\n",
       "         -3.88166421e-01],\n",
       "        [-9.46061077e-02,  1.64318922e+00, -3.15227059e-01,\n",
       "          2.93015193e-01],\n",
       "        [ 4.90830914e-02, -6.38562401e-01, -5.97765143e-01,\n",
       "          1.57700706e-01],\n",
       "        [-5.58379989e-02, -1.32093343e+00,  2.19077744e+00,\n",
       "         -1.90468678e-01]]),\n",
       " array([-0.6376886 ,  0.33012073, -0.02983125, -0.10378818]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
       "      max_iter=1000, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
       "      shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(max_iter = 1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43191.8664,  -1240.0788,  28776.0978, -38699.2622],\n",
       "       [ 13658.04  ,   -182.215 , -22435.7225,    939.6775],\n",
       "       [ 10536.842 ,   -390.834 , -20916.471 ,   4026.029 ],\n",
       "       [ 13972.8092,   -168.8264, -19899.0216,   -597.0016],\n",
       "       [ 17354.1312,   -318.1704, -13864.3376,  -5092.0176],\n",
       "       [ 20430.776 ,   -331.652 , -11704.958 ,  -8254.758 ],\n",
       "       [ 14831.31  ,    -98.635 , -26585.9825,   1466.9175],\n",
       "       [ 13893.3756,   -517.2102, -13741.7113,  -1090.0513],\n",
       "       [ 15592.1108,   -213.9236, -19114.5834,  -1909.4034],\n",
       "       [ 16059.3632,   -121.8494, -22438.4661,  -1333.1461],\n",
       "       [ 13893.0044,     64.9002, -22275.8237,  -1028.2837],\n",
       "       [ 17020.4048,   -303.1466, -13434.5279,  -5152.2479],\n",
       "       [ 16069.7744,   -237.7998, -17829.1237,  -2726.6837],\n",
       "       [ 17920.8844,   -318.0598, -16400.9037,  -4260.9637],\n",
       "       [ 19144.2364,   -388.2188, -12241.1222,  -6682.3822]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is not linearly separable, perceptron algorithm does not converge. The algorithm's accuracy is very poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One vs One\n",
    "\n",
    "We are working with $K$ class, we can use $K(K-1)/2$ binary classifiers for each pair of categories and find the following predictive hypothesis:\n",
    "\n",
    "$$\n",
    "y^{(0,1)}(\\mathbf x) = 1 - y^{(1,0)}(\\mathbf x) = P_{0, 1}(y = 0_{0,1} | \\mathbf x) \\\\\n",
    "y^{(0,2)}(\\mathbf x) = 1 - y^{(1,2)}(\\mathbf x) = P_{0, 2}(y = 0_{0,2} | \\mathbf x) \\\\\n",
    "\\ldots \\\\\n",
    "y^{(K-1,K)}(\\mathbf x) = 1 - y^{(K,K-1)}(\\mathbf x) = P_{K-1}(y = 0_{K-1,K} | \\mathbf x) \\\\\n",
    "$$\n",
    "\n",
    "We can replace $P_{i, j}(y = 0_{i,j} | \\mathbf x) $ in each line by $\\mathbf 1_{i,j}$ if the model is not probabilistic.\n",
    "\n",
    "Finally, we find $k$ that maximizes:\n",
    "\n",
    "$$\n",
    "\\sum_{j \\neq k} y^{(k, j)}(\\mathbf x)\n",
    "$$\n",
    "\n",
    "In perfect model, this sum is $K-1$ for the correct class and $\\leq K - 2$ for other classes. In case where $\\mathbf 1_{i,j}$ is used instead of $P_{i, j}$, this is nothing but majority vote between the $K$ classifiers after a full pairwise classification procedure.\n",
    "\n",
    "This method is called One vs One (OVO)\n",
    "\n",
    "**Comparison with OVR**\n",
    "\n",
    "OVO needs $N(N-1)/2$ classifiers, that is usually longer than OVR (OVA).\n",
    "\n",
    "However, it can solve some non-linearly separable cases that OVR cannot (case 2 and 3 below)\n",
    "\n",
    "<img src=\"F1.png\"></img>\n",
    "<center>Illustration from [2]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python\n",
    "\n",
    "Python supports OVA for the following models:\n",
    "\n",
    "- **sklearn.svm.LinearSVC** (setting decision_function_shape = \"one_vs_one\") (SVM will be presented in next lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='one_vs_one', degree=3, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(decision_function_shape = \"one_vs_one\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59977971,  0.12767047,  0.05004476, -0.57771012, -0.58701971,\n",
       "        -0.00110858],\n",
       "       [ 0.70807823,  0.42621142,  0.06759398, -0.58454003, -0.71385522,\n",
       "        -0.2884136 ],\n",
       "       [ 0.61053631,  0.15126831, -0.25978597, -0.57766006, -0.72335224,\n",
       "        -0.33106619],\n",
       "       [ 0.59977971,  0.10992964,  0.05004472, -0.58520971, -0.58701973,\n",
       "         0.01663222],\n",
       "       [ 0.97115902,  0.93707963,  0.88572994, -0.5776601 , -0.61821883,\n",
       "        -0.07668509],\n",
       "       [ 0.715493  ,  0.38000495,  0.28220759, -0.57766006, -0.60545543,\n",
       "        -0.04581568],\n",
       "       [ 0.59977971, -0.17340517,  0.05004476, -0.70498296, -0.58701971,\n",
       "         0.29996706],\n",
       "       [ 0.59977971,  0.12775458,  0.05004476, -0.57767461, -0.58701971,\n",
       "        -0.00119268],\n",
       "       [ 1.00029759,  0.02133426,  0.94503665, -0.99183384, -0.69502887,\n",
       "         0.93229877],\n",
       "       [ 0.59977971,  0.11102636,  0.05004473, -0.5847461 , -0.58701973,\n",
       "         0.01553551],\n",
       "       [ 0.20829651,  0.12778904,  0.05004476, -0.1865601 , -0.19575374,\n",
       "        -0.00122715],\n",
       "       [ 0.82156224,  0.37293524,  0.57632399, -0.72372147, -0.58701972,\n",
       "         0.34501144],\n",
       "       [ 0.83358982, -0.65482182,  0.59392957, -0.99753247, -0.58705594,\n",
       "         0.99477366],\n",
       "       [ 0.89753784,  0.77654017,  0.75788991, -0.57766006, -0.58707796,\n",
       "        -0.00136806],\n",
       "       [ 0.70123518,  0.34924369,  0.27853255, -0.57766006, -0.59250755,\n",
       "        -0.01450634]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 4, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 1, 1, 3, 4, 3, 1, 2, 1, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hierarchical Methods\n",
    "\n",
    "We can combine advantages of OVR and OVO to describe intermediate methods for multiclass classifications. For example: given categories $\\{0, 1, \\ldots, K-1 = 2^n - 1 \\}$ ($K = 2^n$ categories), using OVO is time infeasible while using OVR may yeild to linear inseparability. We can use OVO in a smarter way:\n",
    "\n",
    "- Classify $\\{0, 1, \\ldots, 2^{n-1}-1\\}$ with $\\{ 2^{n-1}, \\ldots, 2^n -1 \\}$ (first half with second half)\n",
    "- Then for each half, divide into two smaller halves (quarters) and classify them.\n",
    "\n",
    "This method reduces the complexity to $K$ instead of $K^2$ in terms of number of binary classifications. The method is called **hierarchical**. The disadvantage is that we do not know if the division into halves can make the problem linearly separable.\n",
    "\n",
    "We can even improve this complexity to $O(\\log K)$, write every $k \\in \\{0, \\ldots, K-1\\}$ in binary forms: $xxx\\ldots x$ ($n$ digits) and classify by digits. \n",
    "\n",
    "Those methods are generally not implemented in scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Using Multinomial Distribution\n",
    "\n",
    "This method will be detailed in section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multinomial Distribution and Applications\n",
    "\n",
    "## 2.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Binary Distribution tables** (Probabilistic methods)\n",
    "\n",
    "| Model                 | Probabilistic Model   Representation                                                                                                                                                         |                                 |                                                   |Solution                         |                                        |\n",
    "|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|---------------------------------------------------|----------------------------------|----------------------------------------|\n",
    "|                       | Probability Distribution                                                                                                                                                                     | Parameters                      | Optimization Problem                              | Solution                         | Decision Rule                          |\n",
    "| LDA                   | $P(y)$ follow Bernoulli$(\\pi)$,        $p(x|y)$ follow Gaussian$(\\mu_y, \\Sigma)$,      $(x_i, y_i)$ independent                                                                              | $\\pi, \\mu_y, \\Sigma$            | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |      (section 5)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| QDA                   | $P(y)$ follow Bernoulli$(\\pi)$,        $p(x|y)$ follow Gaussian$(\\mu_y, \\Sigma_y)$,      $(x_i, y_i)$ independent                                                                            | $\\pi, \\mu_y, \\Sigma_y$          | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |      (section 6)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| Logistic Regression   | $P(y|x) = \\frac1{1 +   \\exp(-w\\cdot x - b}$,      $y_i$ independent conditionned on $x_i$                                                                                               | $w, b$                          | ML     $p(y_1, \\ldots, y_N| x_1, \\ldots, x_N)$    | Iterative methods                | $1_{w \\cdot x + b > 0}$, argmax_y $P(y|x)$                      |                                                                    |\n",
    "| Gaussian Naïve Bayes  | $P(y)$ follows Bernoulli$(\\pi)$,        The features $x^{(i)}$ of $x$ are independent,      $p(x^{(i)}|y) $ follows Gaussian$(\\mu_y^{(i)}, \\Sigma_y^{(i)})$                                  | $\\pi, \\mu_y^{i}, \\sigma_y^{i}$  | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |     (section 9)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| K Nearest Neighbors   | $P(y) = \\frac{N_y}{N}$,      $P(x|y) = \\frac{K_y}{N_y}|\\mathcal R|$ where $\\mathcal R$ a ball containing   $K$ points                                                                        | Not a parametric model.         |  No optimization problem                          |   No optimization problem         | Majority vote     (as $P(y|x) = K_y/K$ |                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table, we see our probability distribution hypothesis for binary classifiers are of the form \n",
    "\n",
    "$$\n",
    "P(y) \\sim \\mathrm{Bernoulli}(\\pi)\n",
    "$$\n",
    "\n",
    "(for LDA, QDA, GNB, KNN). In case of KNN, $\\pi = N_1/N$.\n",
    "\n",
    "or\n",
    "$$\n",
    "P(y|x) \\sim \\mathrm{Bernoulli}(\\pi(x))\n",
    "$$\n",
    "\n",
    "(for Logistic Regression). In this case, $\\pi(x) = w \\cdot x$.\n",
    "\n",
    "Bernoulli distribution is a random binary variable. We would like to generalize this variable to multivalue case. This yields to **multinomial distribution**.\n",
    "\n",
    "In multinomial distribution, we have a random variable that can get 1 of $K$ values. We will express this random variable as $y = (y_1, y_2, \\ldots, y_K)$ where only one coordinate is 1, the others are 0. The probability that the coordinate $k$ is 1 is $\\pi_k$. Let $\\mathbf \\pi = (\\pi_1, \\ldots, \\pi_K)$. Then:\n",
    "\n",
    "$$\n",
    "p(\\mathbf y) = \\prod_{k=1}^K \\pi_k^{y_k} \n",
    "$$\n",
    "\n",
    "where $\\sum_k \\pi_k = 1$.\n",
    "\n",
    "Now suppose $y$ or $y|x$ follows multinomial distribution of parameter $\\mathbf \\pi$, we can generalize probabilistic multiclass classification models from binary ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Applications\n",
    "\n",
    "### 2.2.1 LDA\n",
    "\n",
    "In multiclass classification, the hypothesis of LDA for prior distribution of categories $\\mathcal C_1, \\ldots, \\mathcal C_K$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf P(\\mathcal C_k) = \\mathbf P(y = (0, \\ldots, 0, 1, 0, \\ldots, 0)) = \\pi_k\n",
    "$$\n",
    "\n",
    "(1 at $k^{th}$ position(\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\n",
    "\\mathbf P(y) = \\prod_{k=1}^K \\pi_k^{y_{(k)}} \n",
    "$$\n",
    "\n",
    "The probability distribution $p(x | \\mathcal C_k)$ still follow Gaussian distribution ($\\mu_k, \\Sigma$):\n",
    "\n",
    "$$\n",
    "p(\\mathbf x|\\mathcal C_k) = \\mathcal N (\\mathbf x_i | \\mu_k, \\Sigma)\n",
    "$$\n",
    "\n",
    "or equivalently\n",
    "$$\n",
    "p(\\mathbf x|y) = \\prod_{k=1}^K \\left( \\mathcal N (\\mathbf x_i | \\mu_k, \\Sigma) \\right)^{y_k}\n",
    "$$\n",
    "\n",
    "The observations are iid, so:\n",
    "\n",
    "$$\n",
    "p(\\mathbf x_1, \\ldots, \\mathbf x_N, y_1, \\ldots, y_N) = \\prod_{i=1}^{N} \\prod_{k=1}^{K}  \\left[ \\pi_k \\delta_1 \\mathcal N (\\mathbf x_i | \\mu_k, \\Sigma)\\right]^{y_{i,k}} \n",
    "$$\n",
    "\n",
    "where $\\pi_1, \\ldots, \\pi_k$.\n",
    "\n",
    "The negative log likelihood is now\n",
    "\n",
    "$$\n",
    "L(\\pi_1, \\ldots, \\pi_K, \\mu_0, \\ldots, \\mu_K) = -\\sum_{i=1}^N \\sum_{k=1}^K (y_{i,k}\\log \\pi_k + y_{i,k} \\log \\mathcal N(\\mathbf x_i | \\mu_k, \\Sigma))\n",
    "$$\n",
    "\n",
    "where $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "Maximum likelihood solution of $L$ for $\\pi_k, \\mu_k, \\Sigma$ ($k=1, \\ldots, K$) can be obtained similarly:\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}N\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac1{N_k}\\sum_{\\mathcal C_k} \\mathbf x_i\n",
    "$$\n",
    "\n",
    "for $k = 1, \\ldots, K$,\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_k \\frac{N_k}N S_k\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "S_k = \\frac1{N_k} \\sum_{\\mathcal C_k} (\\mathbf x_i - \\mu_k)(\\mathbf x_i - \\mu_k)^t, \\qquad k = 1, \\ldots, K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 QDA\n",
    "\n",
    "QDA for multiclass classification is similar to LDA, but do not share the same covariance matrix. So the hypothesis is:\n",
    "\n",
    "$$\n",
    "\\mathbf P(y) = \\prod_{k=1}^K \\pi_k^{y_{(k)}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf x|y) = \\prod_{k=1}^K \\left( \\mathcal N (\\mathbf x_i | \\mu_k, \\Sigma_k) \\right)^{y_k}\n",
    "$$\n",
    "\n",
    "We want to maximize\n",
    "$$\n",
    "p(\\mathbf x_1, \\ldots, \\mathbf x_N, y_1, \\ldots, y_N) = \\prod_{i=1}^{N} \\prod_{k=1}^{K}  \\left[ \\pi_k \\delta_1 \\mathcal N (\\mathbf x_i | \\mu_k, \\Sigma)\\right]^{y_{i,k}} \n",
    "$$\n",
    "\n",
    "The solution is:\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}N\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac1{N_k}\\sum_{\\mathcal C_k} \\mathbf x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac1{N_k} \\sum_{\\mathcal C_k} (\\mathbf x_i - \\mu_k)(\\mathbf x_i - \\mu_k)^t\n",
    "$$\n",
    "\n",
    "for $k = 1, \\ldots, K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes is QDA where features of $\\mathbf x$ are mutually independent.\n",
    "\n",
    "The hypothesis is:\n",
    "$$\n",
    "\\mathbf P(y) = \\prod_{k=1}^K \\pi_k^{y_{(k)}}\n",
    "$$\n",
    "\n",
    "For each feature $i$ of $\\mathbf x = (x^{(1)}, \\ldots, x^{(D)})$.\n",
    "$$\n",
    "P(x^{(i)} | y = \\mathcal C_k, x^{(1)}, \\dots, x^{({i-1})}, x^{({i+1})}, \\dots, x^{(D)}) = P(x^{(i)} | \\mathcal C_k)  = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp\\left(-\\frac{(x^{(i)} - \\mu_k)^2}{2\\sigma^2_k}\\right)\n",
    "$$\n",
    "\n",
    "We maximize:\n",
    "$$\n",
    "\\prod_{n=1}^N \\prod_{k=1}^K \\left( p(y_{n,k}) \\prod_{i=1}^{D} p(x_n^{(i)} \\mid y_{n,k}) \\right)\n",
    "$$\n",
    "\n",
    "The solution is:\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}N\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac1{N_k}\\sum_{\\mathcal C_k} \\mathbf x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\sigma_k^{(i)}) ^{2}= \\frac {1}{|\\mathcal C_k|} \\sum _{y_n = k}\\left( x_n^{(i)} -\\mu_k^{(i)} \\right)^2 \n",
    "$$\n",
    "\n",
    "for $k = 1, \\ldots, K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 K-Nearest Neighbors\n",
    "\n",
    "Nothing changes wrt binary case. We still deduce the rule of majority vote.\n",
    "\n",
    "$$\n",
    "y = \\arg\\max_k \\frac{K_k}K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Logistic Regression\n",
    "\n",
    "In multiclass classification, we suppose each class correspond to a vector $\\mathbf w_{(k)}$, $y | \\mathbf x$ follows:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf x) = \\frac{\\exp(\\mathbf w_{(k)} \\cdot \\mathbf x)}{\\sum_{j=1}^K \\exp(\\mathbf w_{(j)} \\cdot \\mathbf x)}\n",
    "$$\n",
    "\n",
    "By iid hypothesis,\n",
    "\n",
    "$$\n",
    "p(y_1, \\ldots, y_N | \\mathbf x_1, \\ldots, \\mathbf x_N) = \\prod_{n=1}^N \\prod_{k=1}^K p(\\mathcal C_k | \\mathbf x_n)^{y_{(n,k)}}\n",
    "$$\n",
    "\n",
    "This optimization is again, solved by iterative methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Summary\n",
    "\n",
    "** Multiclass Classification models** (Probabilistic methods)\n",
    "\n",
    "| Model                 | Probabilistic Model   Representation                                                                                                                                                         |                                 |                                                   |Solution                         |                                        |\n",
    "|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|---------------------------------------------------|----------------------------------|----------------------------------------|\n",
    "|                       | Probability Distribution                                                                                                                                                                     | Parameters                      | Optimization Problem                              | Solution                         | Decision Rule                          |\n",
    "| LDA                   | $P(y)$ follow Multinomial$(\\pi)$,        $p(x|y)$ follow Gaussian$(\\mu_y, \\Sigma)$,      $(x_i, y_i)$ independent                                                                              | $\\pi, \\mu_y, \\Sigma$            | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |      (section 5)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| QDA                   | $P(y)$ follow Multinomial$(\\pi)$,        $p(x|y)$ follow Gaussian$(\\mu_y, \\Sigma_y)$,      $(x_i, y_i)$ independent                                                                            | $\\pi, \\mu_y, \\Sigma_y$          | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |      (section 6)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| Logistic Regression   | $P(\\mathcal C_k|x) = \\frac{\\exp(-w_{(k)}\\cdot x - b_k)}{\\sum_j   \\exp(-w_{(j)}\\cdot x - b_j})$,      $y_i$ independent conditionned on $x_i$                                                                                               | $w, b$                          | ML     $p(y_1, \\ldots, y_N| x_1, \\ldots, x_N)$    | Iterative methods                | $1_{w \\cdot x + b > 0}$, argmax_y $P(y|x)$                      |                                                                    |\n",
    "| Gaussian Naïve Bayes  | $P(y)$ follows Bernoulli$(\\pi)$,        The features $x^{(i)}$ of $x$ are independent,      $p(x^{(i)}|y) $ follows Gaussian$(\\mu_y^{(i)}, \\Sigma_y^{(i)})$                                  | $\\pi, \\mu_y^{i}, \\sigma_y^{i}$  | ML     $p(x_1, \\ldots, x_N, y_1, \\ldots, y_N)$    |     (section 9)                 | argmax_y $P(y|x)$                      |                                                                    |\n",
    "| K Nearest Neighbors   | $P(y) = \\frac{N_y}{N}$,      $P(x|y) = \\frac{K_y}{N_y}|\\mathcal R|$ where $\\mathcal R$ a ball containing   $K$ points                                                                        | Not a parametric model.         |  No optimization problem                          |   No optimization problem         | Majority vote     (as $P(y|x) = K_y/K$ |                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementation in Python\n",
    "\n",
    "Methods based on multinomial distribution are implemented in many classes of scikit learn, including:\n",
    "\n",
    "- **sklearn.naive_bayes.GaussianNB**\n",
    "- **sklearn.neighbors.KNeighborsClassifier**\n",
    "- **sklearn.discriminant_analysis.LinearDiscriminantAnalysis**\n",
    "- **sklearn.svm.LinearSVC** (setting multi_class=”crammer_singer”)\n",
    "- **sklearn.linear_model.LogisticRegression** (setting multi_class='multinomial')\n",
    "- **sklearn.linear_model.LogisticRegressionCV** (setting multi_class='multinomial')\n",
    "- **sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis**\n",
    "\n",
    "**Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=True, tol=0.0001)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.73268973e-01, 5.01315461e-07, 5.26730526e-01, 3.39960053e-11],\n",
       "       [2.52863395e-01, 4.76790741e-09, 7.46123283e-01, 1.01331738e-03],\n",
       "       [2.41348548e-07, 3.11014272e-21, 2.15035414e-03, 9.97849405e-01],\n",
       "       [6.96744300e-02, 9.97879282e-06, 9.27388285e-01, 2.92730665e-03],\n",
       "       [4.30177654e-01, 2.68249342e-06, 5.69819638e-01, 2.53215010e-08],\n",
       "       [1.43999671e-01, 1.37561459e-05, 8.55875888e-01, 1.10684110e-04],\n",
       "       [7.95990565e-01, 2.25132079e-09, 2.04009422e-01, 1.01315202e-08],\n",
       "       [1.99438885e-07, 9.10404389e-20, 4.60877060e-03, 9.95391030e-01],\n",
       "       [4.02143431e-01, 4.40021081e-07, 5.97851099e-01, 5.02948690e-06],\n",
       "       [6.14979825e-01, 2.78517161e-05, 3.84992323e-01, 1.51158263e-11],\n",
       "       [3.50845231e-01, 5.18565037e-01, 1.30589732e-01, 1.21752774e-11],\n",
       "       [3.48429371e-01, 4.94119567e-05, 6.51521191e-01, 2.64027786e-08],\n",
       "       [3.51780578e-01, 6.99214146e-07, 6.48214725e-01, 3.99792170e-06],\n",
       "       [5.45943207e-01, 1.64429460e-09, 4.54055935e-01, 8.56014461e-07],\n",
       "       [1.35963994e-01, 1.06008009e-08, 8.63946882e-01, 8.91135524e-05]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 3, 3, 1, 4, 3, 1, 2, 3, 3, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 1, 1, 3, 4, 3, 1, 2, 1, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34090909, 0.09090909, 0.25      , 0.31818182])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165.46666667,   7.53333333,   7.35333333,   0.77133333],\n",
       "       [ 80.5       ,   5.925     ,   4.325     ,   0.7975    ],\n",
       "       [200.54545455,   7.57272727,   7.95454545,   0.78090909],\n",
       "       [142.14285714,   6.40714286,   8.65714286,   0.72      ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.68170852e+03,  1.98174095e+01,  2.10894343e+01,\n",
       "        -3.47495179e-01],\n",
       "       [ 1.98174095e+01,  2.88453119e-01,  2.61624606e-01,\n",
       "        -1.20714532e-02],\n",
       "       [ 2.10894343e+01,  2.61624606e-01,  4.54236177e-01,\n",
       "        -3.09573003e-05],\n",
       "       [-3.47495179e-01, -1.20714532e-02, -3.09573003e-05,\n",
       "         4.64407369e-03]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.covariance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.73268973e-01, 5.01315461e-07, 5.26730526e-01, 3.39960053e-11],\n",
       "       [2.52863395e-01, 4.76790741e-09, 7.46123283e-01, 1.01331738e-03],\n",
       "       [2.41348548e-07, 3.11014272e-21, 2.15035414e-03, 9.97849405e-01],\n",
       "       [6.96744300e-02, 9.97879282e-06, 9.27388285e-01, 2.92730665e-03],\n",
       "       [4.30177654e-01, 2.68249342e-06, 5.69819638e-01, 2.53215010e-08],\n",
       "       [1.43999671e-01, 1.37561459e-05, 8.55875888e-01, 1.10684110e-04],\n",
       "       [7.95990565e-01, 2.25132079e-09, 2.04009422e-01, 1.01315202e-08],\n",
       "       [1.99438885e-07, 9.10404389e-20, 4.60877060e-03, 9.95391030e-01],\n",
       "       [4.02143431e-01, 4.40021081e-07, 5.97851099e-01, 5.02948690e-06],\n",
       "       [6.14979825e-01, 2.78517161e-05, 3.84992323e-01, 1.51158263e-11],\n",
       "       [3.50845231e-01, 5.18565037e-01, 1.30589732e-01, 1.21752774e-11],\n",
       "       [3.48429371e-01, 4.94119567e-05, 6.51521191e-01, 2.64027786e-08],\n",
       "       [3.51780578e-01, 6.99214146e-07, 6.48214725e-01, 3.99792170e-06],\n",
       "       [5.45943207e-01, 1.64429460e-09, 4.54055935e-01, 8.56014461e-07],\n",
       "       [1.35963994e-01, 1.06008009e-08, 8.63946882e-01, 8.91135524e-05]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 3, 3, 1, 4, 3, 1, 2, 3, 3, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, 3, 1, 1, 3, 4, 3, 1, 2, 1, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regularization in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation of Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2\n",
    "\n",
    "[2] https://machinelearningcoban.com/2017/02/11/binaryclassifiers/#-binary-classifiers-cho-multi-class-classification-problems\n",
    "\n",
    "[3] https://machinelearningcoban.com/2017/08/31/evaluation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
