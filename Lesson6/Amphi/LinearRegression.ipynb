{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 6 - Regression [1] - Linear Regression, Polynomial Regression, Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression problem, we would like to predict values of a variable $y$ as a function of some variable $\\mathbf X \\in \\mathbf R^D$. \n",
    "\n",
    "Let $\\mathbf X_1, \\ldots, \\mathbf X_N$ and $y_1, \\ldots, y_N$ be some observations of $\\mathbf X$ and $y$, respectively. We want to define a function $g(\\mathbf X)$ to describe the relation between $y$ and $g(\\mathbf X)$, hopefully $y \\approx g(\\mathbf X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a **loss function** $L(g, \\mathbf X, y)$ which has small value when $f(\\mathbf X) \\approx y$ and greater value when $f(\\mathbf X)$ is far from $y$.  \n",
    "\n",
    "In regression, one of the most common choices is the **square loss**. It is convenient for differentiation calculus.\n",
    "\n",
    "**Square loss (with respect to the estimation of $y$ by $g(\\mathbf X)$**\n",
    "$$ L(g, \\mathbf X, y) = \\vert y - g(\\mathbf X) \\vert^2 $$\n",
    "\n",
    "Suppose that $g(\\mathbf X)$ is a good prediction of $y$, then $g(\\mathbf X_i) \\approx y_i$ for $N$ observations $(\\mathbf X_i, y_i), i = 1, \\ldots, N$. In practice, we can define the loss with respect to the estimation of $y_1, \\ldots, y_N$ by $g(\\mathbf X_1), \\ldots, g(\\mathbf X_N)$ as the sum of square loss on each observation.\n",
    "\n",
    "**Square loss (for $ N $ observations)**\n",
    "$$ L(g, \\mathbf X_1, \\ldots, \\mathbf X_N, y_1, \\ldots, y_N) = \\sum_{n=1}^N \\vert y_n - g(\\mathbf X_n) \\vert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, the model is called linear if $g(\\mathbf X)$ is of the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w + b\n",
    "$$\n",
    "where $\\mathbf w \\in \\mathbf R^D$, $\\mathbf b \\in \\mathbf R$.\n",
    "\n",
    "By adding a new coordinate to variable $\\mathbf X$ if necessary, we can suppose that the last coordinate of $\\mathbf X$ is always 1. Then the linear model have the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w\n",
    "$$\n",
    "Here $b$ in the first representation become the last coordinate of $\\mathbf w$.\n",
    "\n",
    "Hence, without loss of generality, we will use $g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w$ as the general form of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Minimizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the square loss as our loss function (a criterion to evaluate which model is better), linear model as our model, and the observation $(\\mathbf X_n, y_n)_{n = 1, \\ldots, N}$ as training data, then the evident strategy is to find $\\mathbf w$ that minimizes the loss function over the training data. The problems becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathbf y = (y_1, \\ldots, y_N)^t$ denote the vector in $\\mathbf R^N$ whose coordinates are the $N$ observations of $y$, and $\\mathbf \\Phi$ denote the matrix in $\\mathbf R^{N \\times D}$ whose rows are $\\mathbf X_n^t$, the $N$ observations of $X$.\n",
    "\n",
    "Then $\\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2$ becomes $\\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2$. The problem becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathcal L(\\mathbf w) = \\Vert \\mathbf y - \\mathbf \\Phi \\cdot \\mathbf w \\Vert^2$. This is a function $\\mathbf R^D \\to \\mathbf R$, convex in $\\mathbf w$, hence a local minimum (if exists) will be unique and minimize the function.\n",
    "\n",
    "The minimum can be found by solving:\n",
    "$$\n",
    "\\nabla_{\\mathbf w} \\mathcal L = \\mathbf 0 \\Leftrightarrow 2\\mathbf \\Phi^t(\\mathbf \\Phi \\mathbf w - \\mathbf y) = 0\n",
    "$$\n",
    "$$\n",
    "\\Leftrightarrow \\mathbf \\Phi^t \\mathbf \\Phi \\mathbf w = \\mathbf \\Phi^t \\mathbf y\n",
    "$$\n",
    "\n",
    "In case $\\mathbf \\Phi^t \\mathbf \\Phi$ invertible, the solution is\n",
    "$$\n",
    "\\hat{\\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{-1} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Phi^t\\Phi$ is not invertible, the inverse can be replace by the (Moore-Penrose) pseudo inverse matrix or any generalized pseudo inverse.\n",
    "$$\n",
    "\\hat{ \\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{+} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The Moore-Penrose pseudo inverse matrix in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **numpy.linalg.pinv** to find the pseudo inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4]\n",
      " [ 3  5  7]\n",
      " [ 4  7 10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, 1], [1, 2, 3]])\n",
    "y = np.array([2, 3])\n",
    "print X.transpose().dot(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.inv(X.transpose().dot(X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.02777778  0.44444444 -1.13888889]\n",
      " [ 0.44444444  0.11111111 -0.22222222]\n",
      " [-1.13888889 -0.22222222  0.69444444]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.pinv(X.transpose().dot(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.16666667  0.66666667  0.16666667]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.pinv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.   8.  11.]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(X)).dot(w) #Phi^t Phi w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  8 11]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(y)) #Phi^t y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution in closed form can be found in $O(ND^2)$ (case $D << N$) or $O(D^3)$ (case $N << D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Probabilistic Model for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hypothesis\n",
    "\n",
    "Suppose that the random variable $\\mathbf X \\in \\mathbf R^D$ and the random variable $y$ satisfying:\n",
    "\n",
    "- The distribution of $\\mathbf X$ is arbitrary.\n",
    "- \n",
    "$$\n",
    "y|\\mathbf X \\sim \\mathcal N \\left(\\mathbf X \\cdot \\mathbf w, \\sigma \\right) \n",
    "$$\n",
    "\n",
    "for some unknown vector $\\mathbf w$. Equivalently,\n",
    "\n",
    "$$\n",
    "p(y | \\mathbf X) = \\frac1{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{|y - \\mathbf X \\cdot \\mathbf w|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Or equivalently,\n",
    "$$\n",
    "y = \\mathbf X \\cdot \\mathbf w + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is some white noise ($\\epsilon \\sim \\mathcal N(0, \\sigma)$), independent of $\\mathbf X$ and independent across observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Likelihood function\n",
    "\n",
    "Consequence of assumption: $y$ is independent across observations, conditional on $\\mathbf X$, i.e., if $(\\mathbf X_i, y_i)$, $(\\mathbf X_j, y_j)$ are observations of $(\\mathbf X, y)$, then\n",
    "\n",
    "$$\n",
    "p(y_i, y_j | \\mathbf X_i, \\mathbf X_j) = p(y_i| \\mathbf X_i)p(y_j | \\mathbf X_j)\n",
    "$$\n",
    "\n",
    "Given $N$ observations of $\\mathbf X, y$ as observations of $N$ iid variables following the same rule as $(\\mathbf X, y)$. Then:\n",
    "$$\n",
    "p(y_1, \\ldots, y_N | \\mathbf X_1, \\ldots, \\mathbf X_N) = p(y_1| \\mathbf X_1) \\ldots p(y_N | \\mathbf X_N) \n",
    "$$\n",
    "\n",
    "This is a function of $\\mathbf w, \\sigma$; is called the likelihood function of observing the data:\n",
    "\n",
    "$$\n",
    "p(y_1, \\ldots, y_N | \\mathbf X_1, \\ldots, \\mathbf X_N; \\mathbf w, \\sigma) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{|y_n - \\mathbf X_n \\cdot \\mathbf w|^2}{2\\sigma^2} \\right) = (2\\pi)^{-N/2} \\sigma^{-N} \\exp\\left( - \\sum_{n=1}^N \\frac{|y_n - \\mathbf X_n \\cdot \\mathbf w|^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "The probabilistic model of linear regression aims to maximize this function with respect to $\\mathbf w, \\sigma$. Equivalently, we minimize the negative **log-likelihood**:\n",
    "\n",
    "$$\n",
    "L(\\mathbf w, \\sigma) = -\\frac N{2} \\log 2\\pi - N \\log \\sigma - \\frac 1 {2\\sigma^2} \\sum_{n=1}^N |y_n - \\mathbf X_n \\cdot \\mathbf w|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The estimators\n",
    "\n",
    "The solution of the minimization problem:\n",
    "$$\n",
    "\\hat{\\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{-1} \\mathbf \\Phi^t \\mathbf y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac1N \\sum_{n=1}^N \\left( y_n - \\mathbf X_n \\cdot \\hat{\\mathbf w} \\right)^2 = \\frac1N \\Vert \\mathbf y - \\mathbf \\Phi \\hat{\\mathbf w} \\Vert^2\n",
    "$$\n",
    "\n",
    "The estimator $\\hat {\\mathbf w}$ is unbiased: $\\mathbf E[\\hat{\\mathbf w}] = \\mathbf w$.\n",
    "\n",
    "The estimator $\\hat {\\sigma}$ is biased: $\\mathbf E [\\hat {\\sigma}^2] = \\frac{N-D}{N} \\sigma^2$. When $\\mathbf E [\\hat {\\sigma}^2] < \\sigma^2$, we say that the $\\sigma$ is underestimated. We can use $\\frac{N}{N-D}\\hat{\\sigma}$ as an unbiased estimator for $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation of Linear Regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate a case $y = \\mathbf X \\cdot \\mathbf w + b + \\epsilon$ where $\\epsilon$ is a white noise (some Gaussian distribution of mean 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "w = [2.0, -1.0, 1.0, 0, 3.5]\n",
    "b = -0.5\n",
    "sigma = 0.8\n",
    "\n",
    "#Generate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train/Test Split and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Bias-Variance Negotiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
