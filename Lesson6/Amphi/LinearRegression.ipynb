{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 6 - Regression [1] - Linear Regression, Polynomial Regression, Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression problem, we would like to predict values of a variable $y$ as a function of some variable $\\mathbf X \\in \\mathbf R^D$. \n",
    "\n",
    "Let $\\mathbf X_1, \\ldots, \\mathbf X_N$ and $y_1, \\ldots, y_N$ be some observations of $\\mathbf X$ and $y$, respectively. We want to define a function $g(\\mathbf X)$ to describe the relation between $y$ and $g(\\mathbf X)$, hopefully $y \\approx g(\\mathbf X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a **loss function** $L(g, \\mathbf X, y)$ which has small value when $f(\\mathbf X) \\approx y$ and greater value when $f(\\mathbf X)$ is far from $y$.  \n",
    "\n",
    "In regression, one of the most common choices is the **square loss**. It is convenient for differentiation calculus.\n",
    "\n",
    "**Quadratic loss (with respect to the estimation of $y$ by $g(\\mathbf X)$**\n",
    "$$ L(g, \\mathbf X, y) = \\vert y - g(\\mathbf X) \\vert^2 $$\n",
    "\n",
    "Suppose that $g(\\mathbf X)$ is a good prediction of $y$, then $g(\\mathbf X_i) \\approx y_i$ for $N$ observations $(\\mathbf X_i, y_i), i = 1, \\ldots, N$. In practice, we can define the loss with respect to the estimation of $y_1, \\ldots, y_N$ by $g(\\mathbf X_1), \\ldots, g(\\mathbf X_N)$ as the sum of square loss on each observation.\n",
    "\n",
    "**Quadratic loss (for $ N $ observations)**\n",
    "$$ L(g, \\mathbf X_1, \\ldots, \\mathbf X_N, y_1, \\ldots, y_N) = \\sum_{n=1}^N \\vert y_n - g(\\mathbf X_n) \\vert^2 $$\n",
    "\n",
    "This loss is also called the squared error/squared loss function.\n",
    "Its mean $$\\frac1N L(g, \\mathbf X_1, \\ldots, \\mathbf X_N, y_1, \\ldots, y_N) $$ is called the **mean-squared error**.\n",
    "Its square root $$\\sqrt {\\frac1N L(g, \\mathbf X_1, \\ldots, \\mathbf X_N, y_1, \\ldots, y_N) }$$ is called the **root-mean-squared error** (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, the model is called linear if $g(\\mathbf X)$ is of the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w + b\n",
    "$$\n",
    "where $\\mathbf w \\in \\mathbf R^D$, $\\mathbf b \\in \\mathbf R$.\n",
    "\n",
    "By adding a new coordinate to variable $\\mathbf X$ if necessary, we can suppose that the last coordinate of $\\mathbf X$ is always 1. Then the linear model have the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w\n",
    "$$\n",
    "Here $b$ in the first representation become the last coordinate of $\\mathbf w$.\n",
    "\n",
    "Hence, without loss of generality, we will use $g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w$ as the general form of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Minimizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the square loss as our loss function (a criterion to evaluate which model is better), linear model as our model, and the observation $(\\mathbf X_n, y_n)_{n = 1, \\ldots, N}$ as training data, then the evident strategy is to find $\\mathbf w$ that minimizes the loss function over the training data. The problems becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathbf y = (y_1, \\ldots, y_N)^t$ denote the vector in $\\mathbf R^N$ whose coordinates are the $N$ observations of $y$, and $\\mathbf \\Phi$ denote the matrix in $\\mathbf R^{N \\times D}$ whose rows are $\\mathbf X_n^t$, the $N$ observations of $X$.\n",
    "\n",
    "Then $\\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2$ becomes $\\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2$. The problem becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathcal L(\\mathbf w) = \\Vert \\mathbf y - \\mathbf \\Phi \\cdot \\mathbf w \\Vert^2$. This is a function $\\mathbf R^D \\to \\mathbf R$, convex in $\\mathbf w$, hence a local minimum (if exists) will be unique and minimize the function.\n",
    "\n",
    "The minimum can be found by solving:\n",
    "$$\n",
    "\\nabla_{\\mathbf w} \\mathcal L = \\mathbf 0 \\Leftrightarrow 2\\mathbf \\Phi^t(\\mathbf \\Phi \\mathbf w - \\mathbf y) = 0\n",
    "$$\n",
    "$$\n",
    "\\Leftrightarrow \\mathbf \\Phi^t \\mathbf \\Phi \\mathbf w = \\mathbf \\Phi^t \\mathbf y\n",
    "$$\n",
    "\n",
    "In case $\\mathbf \\Phi^t \\mathbf \\Phi$ invertible, the solution is\n",
    "$$\n",
    "\\hat{\\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{-1} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Phi^t\\Phi$ is not invertible, the inverse can be replace by the (Moore-Penrose) pseudo inverse matrix or any generalized pseudo inverse.\n",
    "$$\n",
    "\\hat{ \\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{+} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The Moore-Penrose pseudo inverse matrix in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **numpy.linalg.pinv** to find the pseudo inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4]\n",
      " [ 3  5  7]\n",
      " [ 4  7 10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, 1], [1, 2, 3]])\n",
    "y = np.array([2, 3])\n",
    "print X.transpose().dot(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.inv(X.transpose().dot(X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.02777778  0.44444444 -1.13888889]\n",
      " [ 0.44444444  0.11111111 -0.22222222]\n",
      " [-1.13888889 -0.22222222  0.69444444]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.pinv(X.transpose().dot(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.16666667  0.66666667  0.16666667]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.pinv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.   8.  11.]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(X)).dot(w) #Phi^t Phi w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  8 11]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(y)) #Phi^t y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution in closed form can be found in $O(ND^2)$ (case $D << N$) or $O(D^3)$ (case $N << D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Probabilistic Model for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hypothesis\n",
    "\n",
    "Suppose that the random variable $\\mathbf X \\in \\mathbf R^D$ and the random variable $y$ satisfying:\n",
    "\n",
    "- The distribution of $\\mathbf X$ is arbitrary.\n",
    "- \n",
    "$$\n",
    "y|\\mathbf X \\sim \\mathcal N \\left(\\mathbf X \\cdot \\mathbf w, \\sigma \\right) \n",
    "$$\n",
    "\n",
    "for some unknown vector $\\mathbf w$. Equivalently,\n",
    "\n",
    "$$\n",
    "p(y | \\mathbf X) = \\frac1{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{|y - \\mathbf X \\cdot \\mathbf w|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Or equivalently,\n",
    "$$\n",
    "y = \\mathbf X \\cdot \\mathbf w + \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is some white noise ($\\epsilon \\sim \\mathcal N(0, \\sigma)$), independent of $\\mathbf X$ and independent across observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Likelihood function\n",
    "\n",
    "Consequence of assumption: $y$ is independent across observations, conditional on $\\mathbf X$, i.e., if $(\\mathbf X_i, y_i)$, $(\\mathbf X_j, y_j)$ are observations of $(\\mathbf X, y)$, then\n",
    "\n",
    "$$\n",
    "p(y_i, y_j | \\mathbf X_i, \\mathbf X_j) = p(y_i| \\mathbf X_i)p(y_j | \\mathbf X_j)\n",
    "$$\n",
    "\n",
    "Given $N$ observations of $\\mathbf X, y$ as observations of $N$ iid variables following the same rule as $(\\mathbf X, y)$. Then:\n",
    "$$\n",
    "p(y_1, \\ldots, y_N | \\mathbf X_1, \\ldots, \\mathbf X_N) = p(y_1| \\mathbf X_1) \\ldots p(y_N | \\mathbf X_N) \n",
    "$$\n",
    "\n",
    "This is a function of $\\mathbf w, \\sigma$; is called the likelihood function of observing the data:\n",
    "\n",
    "$$\n",
    "p(y_1, \\ldots, y_N | \\mathbf X_1, \\ldots, \\mathbf X_N; \\mathbf w, \\sigma) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{|y_n - \\mathbf X_n \\cdot \\mathbf w|^2}{2\\sigma^2} \\right) = (2\\pi)^{-N/2} \\sigma^{-N} \\exp\\left( - \\sum_{n=1}^N \\frac{|y_n - \\mathbf X_n \\cdot \\mathbf w|^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "The probabilistic model of linear regression aims to maximize this function with respect to $\\mathbf w, \\sigma$. Equivalently, we minimize the negative **log-likelihood**:\n",
    "\n",
    "$$\n",
    "L(\\mathbf w, \\sigma) = -\\frac N{2} \\log 2\\pi - N \\log \\sigma - \\frac 1 {2\\sigma^2} \\sum_{n=1}^N |y_n - \\mathbf X_n \\cdot \\mathbf w|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The estimators\n",
    "\n",
    "The solution of the minimization problem can be found by deriving the function $L$ wrt $\\mathbf w$, $\\mathbf \\sigma$. This is a convex function in $\\mathbf w$.\n",
    "$$\n",
    "\\hat{\\mathbf w} = (\\mathbf \\Phi^t \\mathbf \\Phi)^{-1} \\mathbf \\Phi^t \\mathbf y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac1N \\sum_{n=1}^N \\left( y_n - \\mathbf X_n \\cdot \\hat{\\mathbf w} \\right)^2 = \\frac1N \\Vert \\mathbf y - \\mathbf \\Phi \\hat{\\mathbf w} \\Vert^2\n",
    "$$\n",
    "\n",
    "The estimator $\\hat {\\mathbf w}$ is unbiased: $\\mathbf E[\\hat{\\mathbf w}] = \\mathbf w$.\n",
    "\n",
    "The estimator $\\hat {\\sigma}$ is biased: $\\mathbf E [\\hat {\\sigma}^2] = \\frac{N-D}{N} \\sigma^2$. When $\\mathbf E [\\hat {\\sigma}^2] < \\sigma^2$, we say that the $\\sigma$ is underestimated. We can use $\\frac{N}{N-D}\\hat{\\sigma}$ as an unbiased estimator for $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation of Linear Regression in scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit learn, linear regression model is implemented by **`LinearRegression`** class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "**Parameters:**\n",
    "- `fit_intercept`: `True` when the intercept (coefficient of degree 0 is 0)\n",
    "- `normalize`: `True` if we want to normalize the columns\n",
    "\n",
    "**Attributes:**\n",
    "- `coef_`\n",
    "- `intercept_`\n",
    "\n",
    "**Methods:**\n",
    "- `fit`\n",
    "- `predict`\n",
    "- `score`: The R2-score (Coefficient of determination) evaluated on test set.\n",
    "\n",
    "**Remind**:\n",
    "R2-score: https://fr.wikipedia.org/wiki/Coefficient_de_d%C3%A9termination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate a case $y = \\mathbf X \\cdot \\mathbf w + b + \\epsilon$ where $\\epsilon$ is a white noise (some Gaussian distribution of mean 0 and variance $\\sigma^2 = 0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "TRAIN_TEST_SPLIT_RANDOM_STATE = 0\n",
    "\n",
    "w = np.array([1.5, -2.1, 4, 0, -1.3])\n",
    "b = 2\n",
    "sigma = 0.1\n",
    "\n",
    "D = 5\n",
    "N = 10000\n",
    "\n",
    "X = np.random.uniform(-5, 5, [N, D])\n",
    "epsilon = np.random.normal(0, sigma, N)\n",
    "y = X.dot(w) + b + epsilon\n",
    "\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieved the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.49938033e+00, -2.09964527e+00,  4.00031001e+00,  7.13504117e-04,\n",
       "        -1.30053577e+00]), 1.999665613542589)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef_, lr_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.277741</td>\n",
       "      <td>22.668349</td>\n",
       "      <td>13.986498</td>\n",
       "      <td>14.465190</td>\n",
       "      <td>6.347043</td>\n",
       "      <td>30.614259</td>\n",
       "      <td>-2.897176</td>\n",
       "      <td>3.712321</td>\n",
       "      <td>6.841166</td>\n",
       "      <td>-2.561723</td>\n",
       "      <td>...</td>\n",
       "      <td>15.815272</td>\n",
       "      <td>9.788053</td>\n",
       "      <td>13.152335</td>\n",
       "      <td>17.614369</td>\n",
       "      <td>-4.396698</td>\n",
       "      <td>-6.814206</td>\n",
       "      <td>18.662390</td>\n",
       "      <td>-1.90517</td>\n",
       "      <td>-1.540044</td>\n",
       "      <td>-12.746520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.317423</td>\n",
       "      <td>22.688948</td>\n",
       "      <td>14.070902</td>\n",
       "      <td>14.378545</td>\n",
       "      <td>6.320241</td>\n",
       "      <td>30.484073</td>\n",
       "      <td>-2.783799</td>\n",
       "      <td>3.726107</td>\n",
       "      <td>6.949122</td>\n",
       "      <td>-2.433810</td>\n",
       "      <td>...</td>\n",
       "      <td>15.687191</td>\n",
       "      <td>10.004899</td>\n",
       "      <td>13.247911</td>\n",
       "      <td>17.633092</td>\n",
       "      <td>-4.367921</td>\n",
       "      <td>-6.652553</td>\n",
       "      <td>18.685393</td>\n",
       "      <td>-1.88928</td>\n",
       "      <td>-1.591966</td>\n",
       "      <td>-12.683399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0          1          2          3         4          5         6     \\\n",
       "0  3.277741  22.668349  13.986498  14.465190  6.347043  30.614259 -2.897176   \n",
       "1  3.317423  22.688948  14.070902  14.378545  6.320241  30.484073 -2.783799   \n",
       "\n",
       "       7         8         9       ...           9990       9991       9992  \\\n",
       "0  3.712321  6.841166 -2.561723    ...      15.815272   9.788053  13.152335   \n",
       "1  3.726107  6.949122 -2.433810    ...      15.687191  10.004899  13.247911   \n",
       "\n",
       "        9993      9994      9995       9996     9997      9998       9999  \n",
       "0  17.614369 -4.396698 -6.814206  18.662390 -1.90517 -1.540044 -12.746520  \n",
       "1  17.633092 -4.367921 -6.652553  18.685393 -1.88928 -1.591966 -12.683399  \n",
       "\n",
       "[2 rows x 10000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr_model.predict(X)\n",
    "import pandas as pd\n",
    "pd.DataFrame([y, y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999951318606377"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09976205156094636"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train/Test Split and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
