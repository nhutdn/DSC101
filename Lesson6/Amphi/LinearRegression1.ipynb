{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 6 - An Introduction to Regression: Linear Regression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In amphi 5, we have already classified basic problems in Machine Learning into supervised and unsupervised learning. We have also looked at examples on Regression and Classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return to regression problem. Suppose that we would like to predict values of a variable $y$ as a function of some variable $\\mathbf X \\in \\mathbf R^D$. Let $\\mathbf X_1, \\ldots, \\mathbf X_N$ and $y_1, \\ldots, y_N$ be values of $\\mathbf X$ and $y$, respectively, in $N$ observations. We want to define a function $g(\\mathbf X)$ to describe the relation between $y$ and $g(\\mathbf X)$, hopefully $y = g(\\mathbf X)$ or $f \\approx g(\\mathbf X)$. \n",
    "\n",
    "How do we check that $g(\\mathbf X)$ is a good prediction of $y$? We should define a **loss function** $L(g, \\mathbf X, y)$ which has small value when $f(\\mathbf X) \\approx y$ and greater value when $f(\\mathbf X)$ is far from $y$.  \n",
    "\n",
    "In regression, one of the most common choices is the **square loss**. It is convenient for differentiation calculus.\n",
    "\n",
    "**Square loss (with respect to the estimation of $y$ by $g(\\mathbf X)$**\n",
    "$$ L(g, \\mathbf X, y) = \\vert y - g(\\mathbf X) \\vert^2 $$\n",
    "\n",
    "Suppose that $g(\\mathbf X)$ is a good prediction of $y$, then $g(\\mathbf X_i) \\approx y_i$ for $N$ observations $(\\mathbf X_i, y_i), i = 1, \\ldots, N$. In practice, we can define the loss with respect to the estimation of $y_1, \\ldots, y_N$ by $g(\\mathbf X_1), \\ldots, g(\\mathbf X_N)$ as the sum of square loss on each observation.\n",
    "\n",
    "**Square loss (for $ N $ observations)**\n",
    "$$ L(g, \\mathbf X_1, \\ldots, \\mathbf X_N, y_1, \\ldots, y_N) = \\sum_{n=1}^N \\vert y_n - g(\\mathbf X_n) \\vert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, the model is called linear if $g(\\mathbf X)$ is of the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w + b\n",
    "$$\n",
    "where $\\mathbf w \\in \\mathbf R^D$, $\\mathbf b \\in \\mathbf R$.\n",
    "\n",
    "By adding a new coordinate to variable $\\mathbf X$ if necessary, we can suppose that the last coordinate of $\\mathbf X$ is always 1. Then the linear model have the form:\n",
    "$$\n",
    "g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w\n",
    "$$\n",
    "Here $b$ in the first representation become the last coordinate of $\\mathbf w$.\n",
    "\n",
    "Hence, without loss of generality, we will use $g(\\mathbf X) = \\mathbf X \\cdot \\mathbf w$ as the general form of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Minimizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the square loss as our loss function (a criterion to evaluate which model is better), linear model as our model, and the observation $(\\mathbf X_n, y_n)_{n = 1, \\ldots, N}$ as training data, then the evident strategy is to find $\\mathbf w$ that minimizes the loss function over the training data. The problems becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathbf y = (y_1, \\ldots, y_N)^t$ denote the vector in $\\mathbf R^N$ whose coordinates are the $N$ observations of $y$, and $\\mathbf \\Phi$ denote the matrix in $\\mathbf R^{N \\times D}$ whose rows are $\\mathbf X_n^t$, the $N$ observations of $X$.\n",
    "\n",
    "Then $\\sum_{n=1}^N \\vert y_n - \\mathbf X_n \\cdot \\mathbf w \\vert^2$ becomes $\\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2$. The problem becomes:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\mathbf w \\in \\mathbf R^D} \\Vert \\mathbf y - \\mathbf \\Phi \\mathbf w \\Vert^2\n",
    "$$\n",
    "\n",
    "Let $\\mathcal L(\\mathbf w) = \\Vert \\mathbf y - \\mathbf \\Phi \\cdot \\mathbf w \\Vert^2$. This is a function $\\mathbf R^D \\to \\mathbf R$, convex in $\\mathbf w$, hence a local minimum (if exists) will be unique and minimize the function.\n",
    "\n",
    "The minimum can be found by solving:\n",
    "$$\n",
    "\\nabla_{\\mathbf w} \\mathcal L = \\mathbf 0 \\Leftrightarrow 2\\mathbf \\Phi^t(\\mathbf \\Phi \\mathbf w - \\mathbf y) = 0\n",
    "$$\n",
    "$$\n",
    "\\Leftrightarrow \\mathbf \\Phi^t \\mathbf \\Phi \\mathbf w = \\mathbf \\Phi^t \\mathbf y\n",
    "$$\n",
    "\n",
    "In case $\\mathbf \\Phi^t \\mathbf \\Phi$ invertible, the solution is\n",
    "$$\n",
    "\\mathbf w = (\\mathbf \\Phi^t \\mathbf \\Phi)^{-1} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\Phi^t\\Phi$ is not invertible, the inverse can be replace by the (Moore-Penrose) pseudo inverse matrix or any generalized pseudo inverse.\n",
    "$$\n",
    "\\mathbf w = (\\mathbf \\Phi^t \\mathbf \\Phi)^{+} \\mathbf \\Phi^t \\mathbf y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The Moore-Penrose pseudo inverse matrix in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **numpy.linalg.pinv** to find the pseudo inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4]\n",
      " [ 3  5  7]\n",
      " [ 4  7 10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1, 1], [1, 2, 3]])\n",
    "y = np.array([2, 3])\n",
    "print X.transpose().dot(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.linalg.inv(X.transpose().dot(X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.02777778  0.44444444 -1.13888889]\n",
      " [ 0.44444444  0.11111111 -0.22222222]\n",
      " [-1.13888889 -0.22222222  0.69444444]]\n"
     ]
    }
   ],
   "source": [
    "print np.linalg.pinv(X.transpose().dot(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.16666667  0.66666667  0.16666667]\n"
     ]
    }
   ],
   "source": [
    "w = np.linalg.pinv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.   8.  11.]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(X)).dot(w) #Phi^t Phi w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  8 11]\n"
     ]
    }
   ],
   "source": [
    "print (X.transpose().dot(y)) #Phi^t y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution in closed form can be found in $O(ND^2)$ (case $D << N$) or $O(D^3)$ (case $N << D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
