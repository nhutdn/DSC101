{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three types of ML\n",
    "\n",
    "- Supervised learning: learn to predict an output given an input vector\n",
    " - Regression: Real number, vector of real numbers\n",
    " - Classification: A label (binary or multiclass)\n",
    " \n",
    " Start by choosing a model-class $y = f(\\mathbf x, \\mathbf w)$\n",
    " \n",
    " Fit: minimize some discrepancy between the target output and the actual output produced by the model.\n",
    "- Reinforcement learning: learn to take an action to maximize payoff\n",
    "  - The outout is an action or sequence of actions and the only supervisory signal is an occasional scalar reward\n",
    "  - The goal in selecting each actions is to maximizae the expected sum of the future rewards.\n",
    "  - The reward is typically rewards, it is hard to know where we went wrong\n",
    "  - A scalar reward does not supply much information\n",
    "  \n",
    "- Unsupervised learning: learn to discover good internal representation of the input\n",
    "  - Many reseachers thought that clustering was the only form of unsupervised learning\n",
    "  - The aim of unsupervides learning is not clear. We can say one majour aim is to discover internal representation of the input\n",
    "  - Other goals: Dimensionality reduction, provides an economical representation of the input in terms of learned features, find sensible clusters in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason to study neural computation\n",
    "\n",
    "- To understand how the brain actually works\n",
    "- To under stand a style of parallel computation inspired by neurons and their adaptative connections\n",
    "- To solve pratical problems by using novel learning algorithms inspired by the brain.\n",
    "\n",
    "### A typical cortical neuron\n",
    "\n",
    "- Gross physical structure: one axon that branches, a dendritic tree that collects input from other neurons\n",
    "- Axon typically contact dendritic trees at synapses: a spike of activity in the axon causes charge to be injected into the post-synaptic neuron\n",
    "- There is an axon hillock that generates outgoing spikes whever enough charge has flowed in at synapses to depolarize the cell membrane.\n",
    "- The visual cortex, which is intended to solve image recognition problems, shows a sequence of sectors placed in a hierarchy. Each of these areas receives an input representation, by means of flow signals that connect it to other sectors.\n",
    "\n",
    "### Synapses:\n",
    "- When a spike of activity travels along an axon and arrives at a synapse it causes vesicles of transmitter chemical to be released\n",
    "- The transmitter molecules diffuse across the synaptic cleft and bind to receptor molecules in the membrane of the post-synaptic neuron thus changing their shape.\n",
    "- The effectiveness of the synapse can be changed:\n",
    "    - vary the number of vesicles of transmitter\n",
    "    - vary the number of receptor molecules\n",
    "- Synapses are slow but gave advantages over RAM\n",
    "- The effect of each input line on the neuron is controlled by a synaptic weight, the weight can be positive or negative\n",
    "- The synaptic weighst adapt so that the whole network learns to perform useful computations. We have about $10^{11}$ neuron each with about $10^4$ weights.\n",
    "- Different bits of the cortex do different things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple models of neurons\n",
    "\n",
    "- Linear neuron\n",
    "$$\n",
    "y = w_0 + \\sum_{n=1}^D x_i w_i\n",
    "$$\n",
    "($w_0$: bias, $w_1, \\ldots, w_D$: weight on $i^{th}$ feature).\n",
    "\n",
    "- Binary threshold neuron: compute a weighted sum of the input, send out a finxed size spike of activity if the weighted sum exceeds a threshold.\n",
    "$$\n",
    "y = 1_{w_0 + \\sum_{i=1}^D x_i w_i \\geq 0}\n",
    "$$\n",
    "\n",
    "- Rectified Linear newron (threshold newron)\n",
    "$$\n",
    "y = (w_0 + \\sum_{i=1}^D x_i w_i) 1_{w_0 + \\sum_{i=1}^D x_i w_i \\geq 0}\n",
    "$$\n",
    "\n",
    "- Signoid neurons\n",
    "$$\n",
    "y = \\frac1{1+ \\exp(-w_0 - \\sum_{i=1}^D x_i w_i)}\n",
    "$$\n",
    "\n",
    "- Stochastic binary neurons\n",
    "$$\n",
    "P(y = 1) = \\frac1{1+ \\exp(-w_0 - \\sum_{i=1}^D x_i w_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of NN\n",
    "\n",
    "- Feed-forward: First layer is input, last layer is output, one or more hidden layers. They compute a series of transformations that change the similarities between cases.\n",
    "\n",
    "<img src=\"F1.png\"></img>\n",
    "\n",
    "- Recurrent: Have directed cycles in their connection graph. Can have complicated dynamics and make them very difficult to train. \n",
    "\n",
    "<img src=\"F2.png\"></img>\n",
    "\n",
    "It is natural to model sequential data, it is equivalent to very deep nets with one hidden layer per time slice. They have ability to remember information in the hidden layers for a long time.\n",
    "\n",
    "<img src=\"F3.png\"></img>\n",
    "\n",
    "- Symmetrically connected network: Like recurrent networks but the connections between units are symmetrical (same weight in both directions). More restricted in what they can do because they obey an energy function. For example, they cannot model cycles.\n",
    "\n",
    "### What recurrent neral nets can do \n",
    "\n",
    "- I.Sutskever (2011) trained a special type of RNN to predict the next character in a sequence. After training for a long time on a string of $5\\cdot 10^8$ characters, he got it to generate new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron - The First Generation of Neural Network\n",
    "\n",
    "**Paradigm:**\n",
    "\n",
    "- Learn how to weight each of the feature activations to get a single scalar quantity\n",
    "\n",
    "- If this quantity is above some threshold, decide that the input vector is of class 1, otherwise class 0.\n",
    "\n",
    "**Binary threshold neurons**\n",
    "$$\n",
    "z = w_0 + \\sum_{i=1}^D w_i\n",
    "$$\n",
    "\n",
    "Decision rule\n",
    "$$\n",
    "y = \\begin{cases} 1 \\qquad \\textrm{ if } z \\geq 0; \\\\ 0 \\qquad \\textrm{ if } z < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "In other words,\n",
    "$$\n",
    "y = \\mathbf 1_{w_0 + \\sum_{i=1}^D w_i \\geq 0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The perceptron convergence procedure **\n",
    "\n",
    "- Add an extra component with value 1 to each input vector.\n",
    "- Pick training cases using any policy that ensures every training case will keep getting picked.\n",
    " - If the output unit is correct, do nothing\n",
    " - If the output unit is incorrectly outputs a zero, add the input vector to the weight vector.\n",
    " - If the output unit is incorrectly outpust a 1, subtract the input vector from the weight vector.\n",
    "- This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Weight-space**\n",
    "\n",
    "- The weight space is a vector space where each weight is represented by a dimension. A point in the space represents a particular settings for all the weights. \n",
    "\n",
    "- Assume that we eliminated the threshold (using bias as a new weight), each training case can be a represented as a hyperplane through the origin $w \\cdot x = 0$. So the weight must lie on one side of this hyperplane to get the answer correct. On positive class, $w \\cdot x \\geq 0$, on negative class $w \\cdot x < 0$.\n",
    "\n",
    "- So we need to find a vector $w$ that correctly solve the inequation system $w \\cdot x \\geq 0$ for positive class and $w \\cdot x < 0$ for negative class.\n",
    "\n",
    "- In other words, we have $N$ half-spaces and wish to find a point in the intersection (if any) of those half-spaces. Such an intersection (if exists), is a hyper-cone with its apex at 0. It is a convex set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Why the algorithm works**\n",
    "\n",
    "- Consider the square distance between any feasible weight vector $\\tilde w$ and the current weight vector $w$: $\\Vert w - \\tilde  w\\Vert^2$. A feasible weight vector is a solution of the problem.\n",
    "\n",
    "- We represent this squared distance as the squared distance along the hyperplane of the wrong training case ($d_b^2$) and the squared distance perpendicular to that hyperplane ($d_a^2)$ \n",
    "<img src=\"F4.png\" width=400></img>\n",
    "\n",
    "- We would like to get this distance smaller each time we fix the weight for a wrong training case. But it is wrong.\n",
    "\n",
    "- We define a \"generously feasible\" weight vectors that lie within the feasible region by a margin at least as great as the length of the input vector that defines each constraint plane.\n",
    "\n",
    "- This time, the idea works. Every time the perceptron makes a mistak, the squared distance to all of these generoulsy feasible weight vectors is always decreased by at least the squared length of the update vector.\n",
    "<img src=\"F5.png\" width=400></img>\n",
    "\n",
    "** Informal sketch of proof**\n",
    "\n",
    "- Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance form every weight vector in the \"generously feasible\" region.\n",
    "- The squared distance decreases by at least the squared length of the input vector.\n",
    "- So after a finite number of mistakes, the weight vector must lie in the feasible region if this region exists.\n",
    "\n",
    "**What perceptrons can't do**\n",
    "\n",
    "- XOR problem\n",
    "- Pattern recognition with the same number of pixels as a feature, wrap-around allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
